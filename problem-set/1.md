# Homework 1 - Linear algebra

Due date: May - 18

Student: Carlos Antunis Bonfim da Silva Santos ([crlsantnys@gmail.com](mailto:crlsantnys@gmail.com))

## Exercise 1. Vectors

### item a.

Suppose a linear space $\mathcal{L}$ in which the set $\beta =$ { $\mathbf{e}_i$ } (for $i = 1, ..., n$) is a basis (a LI set which spans $\mathcal{L}$). Let $\mathbf{v} \in \mathcal{L}$ a vector whose is decomposed with coordinates $v^{(i)}$ and $\tilde{v}^{(i)}$ in this basis, thus:

$$
   \mathbf{v} - \mathbf{v} = \sum_i v^{(i)} \mathbf{e}_i - \sum_i \tilde{v}^{(i)} \mathbf{e}_i = \sum_i (v^{(i)} - \tilde{v}^{(i)}) \mathbf{e}_i \text{,}
$$

since the basis is a LI set, the difference between $v^{(i)}$ and $\tilde{v}^{(i)}$ is zero, so the coordinates is unique to a given basis.

### item b.

Given two basis, { $\mathbf{e}_i$ } and { $\mathbf{\tilde{e}}_i$ }, then, since each element of the basis is an element of the linear spaces:

$$
    \forall \mathbf{e}_i, \exists! R^l_i: \mathbf{e}_i = \sum_l R^l_i \mathbf{\tilde{e}}_l\text{,}
$$

and also

$$
    \forall \mathbf{\tilde{e}}_i, \exists! P^j_i: \mathbf{e}_i = \sum_j P^j_i \mathbf{e}_j\text{.}
$$

Taking a vector $\mathbf{v} = \sum_i v^{(i)} \mathbf{e}_i$, then writing each vector in the tilde basis:

$$
    \mathbf{v} = \sum_i v^{(i)} \sum_l R^l_i \mathbf{\tilde{e}}_l\text{,}
$$

so, returning to the original basis:

$$
    \mathbf{v} = \sum_j (\sum_{i,l} R^l_i P^j_l v^{(i)}) \mathbf{e}_j\text{,}
$$

implying, due to the unicity of coordinates, that $\sum_{i,l} R^l_i P^j_l v^{(i)} (= v^{(j)}) = \sum_{i,j} \delta_i^j v^{(j)}$ implying that the $R_i^l = (P^{-1})_i^l$.

### item c.

Let $f: S \mapsto \mathbb{R}$, then:

$$
    \forall i = 1,...,n: f(s_i) = f_i\text{,}
$$

thus, defining $\Phi^i(s_j) = \delta^i_j$ and $\mathcal{F}$ as the set of all functions $S\mapsto\mathbb{R}$,

$$
    \forall f \in \mathcal{F}: f = \sum_i f_i\Phi^i\text{,}
$$

so the set of all $\Phi^i$ is a basis of $\mathcal{F}$, implying that the dimension of the linear space is $n$. Let $\Psi^i$, for $i=1,...,m$, and $\Gamma^i$, for $i = 1, ..., n$ then if $m > n$ for a ser of scalars $x_i$:

$$
    \mathbf{0} = \sum_i x_i\Psi^i = \sum_i^n x_i \sum_j^m A_j^i \Gamma^j = \sum_j^m (\sum_i^n A_j^i x_i) \Gamma^j\text{,}
$$

so, if $m>n$ will exists non-zero $x_i$ in which $\sum_i^n A_j^i x_i = 0$ (the coordinates in another basis). Therefore, given two basis with $m$ and $n$ elements, $m \le n$, however reciprocally $n \le m$ (because each one is a basis for the same linear space), resulting in $m = n$.

## Exercise 2. Dual spaces

### item a.

### item b.

### item c.

## Exercise 3. Tensors

### item a.

### item b.

### item c.

### item d.

## Exercise 4. Quotient spaces

### item a.

### item b.

## Exercise 5. Norms

## Exercise 6. Invariant subspaces

For each one of the matrices, $\mathbb{R}^2$ (every result will be in the space) and { $\mathbf{0}$ } (consequence of the map's linearity) is invariant subspaces, thus each other subspace will be $1-dimensional$, or equivalently will be its eigenspaces.

* For $A_1$, the eigenvalues will be $\pm\sqrt{2}$ (since its characteristic polynomial is $x^2 - 2$), and its respectives eigenspaces will be the subsequent kernels:
   * $ker(A_1 - \sqrt{2}I) = span(\mathbf{e}_1 + \sqrt{2} \mathbf{e}_2 / 2)$;
   * $ker(A_1 + \sqrt{2}I) = span(\mathbf{e}_1 - \sqrt{2} \mathbf{e}_2 / 2)$;
* For $A_2$, the eigenvalues will be $1, 2$ (since its characteristic polynomial is $(1 - x)(2 - x)$), and its respectives eigenspaces will be the subsequent kernels:
   * $ker(A_2 - 1I) = span(\mathbf{e}_1)$;
   * $ker(A_2 - 2I) = span(\mathbf{e}_1 + \mathbf{e}_2 / 2)$;
* For $A_3$, the eigenvalue will be $1$, with algebraic multiplicity 2, (since its characteristic polynomial is $(1 - x)^2$), and its eigenspace will be the subsequent kernel:
   * $ker(A_3 - 1I) = span(\mathbf{e}_1)$;
