# Homework 1 - Linear algebra

Due date: May - 18

Student: Carlos Antunis Bonfim da Silva Santos ([crlsantnys@gmail.com](mailto:crlsantnys@gmail.com))

## Exercise 1. Vectors

### item a.

Suppose a linear space $\mathcal{L}$ in which the set $\beta =$ { $\mathbf{e}_i$ } (for $i = 1, ..., n$) is a basis (a LI set which spans $\mathcal{L}$). Let $\mathbf{v} \in \mathcal{L}$ a vector whose is decomposed with coordinates $v^{(i)}$ and $\tilde{v}^{(i)}$ in this basis, thus:

$$
   \mathbf{v} - \mathbf{v} = \sum_i v^{(i)} \mathbf{e}_i - \sum_i \tilde{v}^{(i)} \mathbf{e}_i = \sum_i (v^{(i)} - \tilde{v}^{(i)}) \mathbf{e}_i \text{,}
$$

since the basis is a LI set, the difference between $v^{(i)}$ and $\tilde{v}^{(i)}$ is zero, so the coordinates is unique to a given basis.

### item b.

Given two basis, { $\mathbf{e}_i$ } and { $\mathbf{\tilde{e}}_i$ }, then, since each element of the basis is an element of the linear spaces:

$$
    \forall \mathbf{e}_i, \exists! R^l_i: \mathbf{e}_i = \sum_l R^l_i \mathbf{\tilde{e}}_l\text{,}
$$

and also

$$
    \forall \mathbf{\tilde{e}}_i, \exists! P^j_i: \mathbf{e}_i = \sum_j P^j_i \mathbf{e}_j\text{.}
$$

Taking a vector $\mathbf{v} = \sum_i v^{(i)} \mathbf{e}_i$, then writing each vector in the tilde basis:

$$
    \mathbf{v} = \sum_i v^{(i)} \sum_l R^l_i \mathbf{\tilde{e}}_l\text{,}
$$

so, returning to the original basis:

$$
    \mathbf{v} = \sum_j (\sum_{i,l} R^l_i P^j_l v^{(i)}) \mathbf{e}_j\text{,}
$$

implying, due to the unicity of coordinates, that $\sum_{i,l} R^l_i P^j_l v^{(i)} (= v^{(j)}) = \sum_{i,j} \delta_i^j v^{(j)}$ implying that the $R_i^l = (P^{-1})_i^l$.

### item c.

Let $f: S \mapsto \mathbb{R}$, then:

$$
    \forall i = 1,...,n: f(s_i) = f_i\text{,}
$$

thus, defining $\Phi^i(s_j) = \delta^i_j$ and $\mathcal{F}$ as the set of all functions $S\mapsto\mathbb{R}$,

$$
    \forall f \in \mathcal{F}: f = \sum_i f_i\Phi^i\text{,}
$$

so the set of all $\Phi^i$ is a basis of $\mathcal{F}$, implying that the dimension of the linear space is $n$. Let $\Psi^i$, for $i=1,...,m$, and $\Gamma^i$, for $i = 1, ..., n$ then if $m > n$ for a ser of scalars $x_i$:

$$
    \mathbf{0} = \sum_i x_i\Psi^i = \sum_i^n x_i \sum_j^m A_j^i \Gamma^j = \sum_j^m (\sum_i^n A_j^i x_i) \Gamma^j\text{,}
$$

so, if $m>n$ will exists non-zero $x_i$ in which $\sum_i^n A_j^i x_i = 0$ (the coordinates in another basis). Therefore, given two basis with $m$ and $n$ elements, $m \le n$, however reciprocally $n \le m$ (because each one is a basis for the same linear space), resulting in $m = n$.

## Exercise 2. Dual spaces

### item a.

Since $V^\ast$ is a set of all linear functionals $\omega: V \mapsto F$, in which $F$ is the field (as real or complex number fields) of $V$, thus by the results of $\omega(\mathbf{v}) \in \mathbf{F}$, immediately follows that the dual space is a linear space (using that $F$ itself is a 1-dimensional linear space over itself, and $(\omega^1 + c \omega^2)(\mathbf{v}) = (\omega^1(\mathbf{v})) + c (\omega^2(\mathbf{v}))$ ). To show that the set of $\theta^i$'s is a basis we only need to show that it's linearly independent, since $dim(V^\ast) = dim(L(V, F)) = dim V \times dim F = dim V$, thus:

$$
   \forall j = 1, ..., n: 0 = \mathbf{0}(\mathbf{e}_j) = \sum_i c_i \theta^i(\mathbf{e}_j) = \sum_i c_i \delta^i_j = c_j
$$

therefore, $c_i = 0$.

### item b.

Let $\mathbf{v} = \sum_i v^i\mathbf{e}_j$, thus:

$$
   \theta^i(\mathbf{v}) = \theta^i(\sum_i v^i\mathbf{e}_j) = \sum_i v^i\theta^i(\mathbf{e}_j) = \sum_i v^i\delta^i_j = v^j\text{.}
$$

### item c.

Since the set is the co-basis (in the double dual space $V^{\ast\ast}$ - since its contravariant) of the basis set $a^i$, its co-basis will be $X_{a^i}(\omega) \equiv \omega(a^i)$ (the evaluation map basis).

## Exercise 3. Tensors

### item a.

Since $A$ is antisymmetric, $A(\mathbf{e}_1, ..., \mathbf{e}_i, ..., \mathbf{e}_i, ... \mathbf{e}_n) = 0$, thus:

$$
    A(\mathbf{e}_1, ..., \mathbf{e}_{n - 1}, \sum_i^{n - 1} c^i \mathbf{e}_i) = \sum_i^{n - 1} c^i A\mathbf{e}_1, ..., \mathbf{e}_{n - 1}, \mathbf{e}_i) = 0\text{,}
$$

so, if $A(,...,) = 0 \Leftrightarrow the set is LD$, thus a set with n elements in which A does not vanish, the set is LI and therefore a basis.

### item b.

The identity is $I = \sum_i \Theta^i(\cdot)\mathbf{e}_i$, since $v^i = \Theta^i(\mathbf{v})$.

### item c.

### item d.

## Exercise 4. Quotient spaces

### item a.

### item b.

## Exercise 5. Norms

## Exercise 6. Invariant subspaces

For each one of the matrices, $\mathbb{R}^2$ (every result will be in the space) and { $\mathbf{0}$ } (consequence of the map's linearity) is invariant subspaces, thus each other subspace will be $1-dimensional$, or equivalently will be its eigenspaces.

* For $A_1$, the eigenvalues will be $\pm\sqrt{2}$ (since its characteristic polynomial is $x^2 - 2$), and its respectives eigenspaces will be the subsequent kernels:
   * $ker(A_1 - \sqrt{2}I) = span(\mathbf{e}_1 + \sqrt{2} \mathbf{e}_2 / 2)$;
   * $ker(A_1 + \sqrt{2}I) = span(\mathbf{e}_1 - \sqrt{2} \mathbf{e}_2 / 2)$;
* For $A_2$, the eigenvalues will be $1, 2$ (since its characteristic polynomial is $(1 - x)(2 - x)$), and its respectives eigenspaces will be the subsequent kernels:
   * $ker(A_2 - 1I) = span(\mathbf{e}_1)$;
   * $ker(A_2 - 2I) = span(\mathbf{e}_1 + \mathbf{e}_2 / 2)$;
* For $A_3$, the eigenvalue will be $1$, with algebraic multiplicity 2, (since its characteristic polynomial is $(1 - x)^2$), and its eigenspace will be the subsequent kernel:
   * $ker(A_3 - 1I) = span(\mathbf{e}_1)$;
